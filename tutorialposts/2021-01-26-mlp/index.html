<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content=fluxml.ai > <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="../../css/script_default.css"> <link rel=stylesheet  href="../../css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"> </script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="../../"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="../../getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="../../blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="../../tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/dev/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="../../gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=content > <div class=container > <h1>Simple multi-layer perceptron</h1> <div class=franklin-content > <p>In this example, we create a simple <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron#:~:text&#61;A&#37;20multilayer&#37;20perceptron&#37;20&#40;MLP&#41;&#37;20is,artificial&#37;20neural&#37;20network&#37;20&#40;ANN&#41;.&amp;text&#61;An&#37;20MLP&#37;20consists&#37;20of&#37;20at,uses&#37;20a&#37;20nonlinear&#37;20activation&#37;20function.">multi-layer perceptron</a> &#40;MLP&#41; that classifies handwritten digits using the MNIST dataset. A MLP consists of at least <em>three layers</em> of stacked perceptrons: Input, hidden, and output. Each neuron of an MLP has parameters &#40;weights and bias&#41; and uses an <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> to compute its output. </p> <p>To run this example, we need the following packages:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Flux, Statistics
<span class=hljs-keyword >using</span> Flux.Data: DataLoader
<span class=hljs-keyword >using</span> Flux: onehotbatch, onecold, logitcrossentropy, throttle, <span class=hljs-meta >@epochs</span>
<span class=hljs-keyword >using</span> Base.Iterators: repeated
<span class=hljs-keyword >using</span> Parameters: <span class=hljs-meta >@with_kw</span>
<span class=hljs-keyword >using</span> CUDA
<span class=hljs-keyword >using</span> MLDatasets
<span class=hljs-keyword >if</span> has_cuda()		<span class=hljs-comment ># Check if CUDA is available</span>
    <span class=hljs-meta >@info</span> <span class=hljs-string >&quot;CUDA is on&quot;</span>
    CUDA.allowscalar(<span class=hljs-literal >false</span>)
<span class=hljs-keyword >end</span></code></pre> <p>We set default values for learning rate, batch size, epochs, and the usage of a GPU &#40;if available&#41; for our model:</p> <pre><code class="julia hljs"><span class=hljs-meta >@with_kw</span> <span class=hljs-keyword >mutable struct</span> Args
    η::<span class=hljs-built_in >Float64</span> = <span class=hljs-number >3e-4</span>       <span class=hljs-comment ># learning rate</span>
    batchsize::<span class=hljs-built_in >Int</span> = <span class=hljs-number >1024</span>   <span class=hljs-comment ># batch size</span>
    epochs::<span class=hljs-built_in >Int</span> = <span class=hljs-number >10</span>        <span class=hljs-comment ># number of epochs</span>
    device::<span class=hljs-built_in >Function</span> = gpu  <span class=hljs-comment ># set as gpu, if gpu available</span>
<span class=hljs-keyword >end</span></code></pre> <p>If a GPU is available on our local system, then Flux uses it for computing the loss and updating the weights and biases when training our model.</p> <h2 id=data ><a href="#data" class=header-anchor >Data</a></h2> <p>We create the function <code>getdata</code> to load the MNIST train and test data sets from <a href="https://juliaml.github.io/MLDatasets.jl/latest/">MLDatasets</a> and prepare them for the training process. In addition, we set mini-batches of the data sets by loading them onto a <a href="https://fluxml.ai/Flux.jl/stable/data/dataloader/#Flux.Data.DataLoader">DataLoader</a> object. </p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> getdata(args)
    <span class=hljs-literal >ENV</span>[<span class=hljs-string >&quot;DATADEPS_ALWAYS_ACCEPT&quot;</span>] = <span class=hljs-string >&quot;true&quot;</span>

    <span class=hljs-comment ># Loading Dataset	</span>
    xtrain, ytrain = MLDatasets.MNIST.traindata(<span class=hljs-built_in >Float32</span>)
    xtest, ytest = MLDatasets.MNIST.testdata(<span class=hljs-built_in >Float32</span>)
	
    <span class=hljs-comment ># Reshape Data in order to flatten each image into a linear array</span>
    xtrain = Flux.flatten(xtrain)
    xtest = Flux.flatten(xtest)

    <span class=hljs-comment ># One-hot-encode the labels</span>
    ytrain, ytest = onehotbatch(ytrain, <span class=hljs-number >0</span>:<span class=hljs-number >9</span>), onehotbatch(ytest, <span class=hljs-number >0</span>:<span class=hljs-number >9</span>)

    <span class=hljs-comment ># Batching</span>
    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=<span class=hljs-literal >true</span>)
    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize)

    <span class=hljs-keyword >return</span> train_data, test_data
<span class=hljs-keyword >end</span></code></pre> <p><code>getdata</code> performs the following steps:</p> <ul> <li><p><strong>Loads MNIST data set:</strong> Loads the train and test set tensors. The shape of train data is <code>28x28x60000</code> and test data is <code>28X28X10000</code>. </p> <li><p><strong>Reshapes the train and test data:</strong> Uses the <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.flatten">flatten</a> function to reshape the train data set into a <code>784x60000</code> array and test data set into a <code>784x10000</code>. Notice that we reshape the data so that we can pass these as arguments for the input layer of our model &#40;a simple MLP expects a vector as an input&#41;.</p> <li><p><strong>One-hot encodes the train and test labels:</strong> Creates a batch of one-hot vectors so we can pass the labels of the data as arguments for the loss function. For this example, we use the <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.logitcrossentropy">logitcrossentropy</a> function and it expects data to be one-hot encoded. </p> <li><p><strong>Creates batches of data:</strong> Creates two DataLoader objects &#40;train and test&#41; that handle data mini-batches of size <code>1024</code> &#40;as defined above&#41;. We create these two objects so that we can pass the entire data set through the loss function at once when training our model. Also, it shuffles the data points during each iteration &#40;<code>shuffle&#61;true</code>&#41;.</p> </ul> <h2 id=model ><a href="#model" class=header-anchor >Model</a></h2> <p>As we mentioned above, a MLP consist of <em>three</em> layers that are fully connected. For this example, we define out model with the following layers and dimensions: </p> <ul> <li><p><strong>Input:</strong> It has <code>784</code> perceptrons &#40;the MNIST image size is <code>28x28</code>&#41;. We flatten the train and test data so that we can pass them as arguments to this layer.</p> <li><p><strong>Hidden:</strong> It has <code>32</code> perceptrons that use the <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.relu">relu</a> activation function.</p> <li><p><strong>Output:</strong> It has <code>10</code> perceptrons that output the model&#39;s prediction or probability that a digit is 0 to 9. </p> </ul> <p>We define our model with the <code>build_model</code> function: </p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> build_model(; imgsize=(<span class=hljs-number >28</span>,<span class=hljs-number >28</span>,<span class=hljs-number >1</span>), nclasses=<span class=hljs-number >10</span>)
    <span class=hljs-keyword >return</span> Chain(
 	    Dense(prod(imgsize), <span class=hljs-number >32</span>, relu),
            Dense(<span class=hljs-number >32</span>, nclasses))
<span class=hljs-keyword >end</span></code></pre> <p>Note that we use the functions <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dense">Dense</a> so that our model is <em>densely</em> &#40;or fully&#41; connected and <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Chain">Chain</a> to chain the computation of the three layers.</p> <h2 id=loss_functions ><a href="#loss_functions" class=header-anchor >Loss functions</a></h2> <p>Now, we define the loss function <code>loss_all</code>. It expects a DataLoader object and the <code>model</code> function we defined aboved as arguments. Notice that this function iterates through the <code>dataloader</code> object in mini-batches and uses the function <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.logitcrossentropy">logitcrossentropy</a> to compute the difference between the predicted and actual values. </p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> loss_all(dataloader, model)
    l = <span class=hljs-number >0f0</span>
    <span class=hljs-keyword >for</span> (x,y) <span class=hljs-keyword >in</span> dataloader
        l += logitcrossentropy(model(x), y)
    <span class=hljs-keyword >end</span>
    l/length(dataloader)
<span class=hljs-keyword >end</span></code></pre> <p>In addition, we define the function &#40;<code>accuracy</code>&#41; to report the accuracy of our model during the training process. To compute the accuray, we need to decode the output of our model using the <a href="https://fluxml.ai/Flux.jl/stable/data/onehot/#Flux.onecold">onecold</a> function. </p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> accuracy(data_loader, model)
    acc = <span class=hljs-number >0</span>
    <span class=hljs-keyword >for</span> (x,y) <span class=hljs-keyword >in</span> data_loader
        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*<span class=hljs-number >1</span> / size(x,<span class=hljs-number >2</span>)
    <span class=hljs-keyword >end</span>
    acc/length(data_loader)
<span class=hljs-keyword >end</span></code></pre> <h2 id=train_our_model ><a href="#train_our_model" class=header-anchor >Train our model</a></h2> <p>Finally, we create the <code>train</code> function that calls the functions we defined and trains the model.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> train(; kws...)
    <span class=hljs-comment ># Initializing Model parameters </span>
    args = Args(; kws...)

    <span class=hljs-comment ># Load Data</span>
    train_data,test_data = getdata(args)

    <span class=hljs-comment ># Construct model</span>
    m = build_model()
    train_data = args.device.(train_data)
    test_data = args.device.(test_data)
    m = args.device(m)
    loss(x,y) = logitcrossentropy(m(x), y)
    
    <span class=hljs-comment >## Training</span>
    evalcb = () -&gt; <span class=hljs-meta >@show</span>(loss_all(train_data, m))
    opt = ADAM(args.η)
		
    <span class=hljs-meta >@epochs</span> args.epochs Flux.train!(loss, params(m), train_data, opt, cb = evalcb)

    <span class=hljs-meta >@show</span> accuracy(train_data, m)

    <span class=hljs-meta >@show</span> accuracy(test_data, m)
<span class=hljs-keyword >end</span></code></pre> <p><code>train</code> performs the following steps:</p> <ul> <li><p><strong>Initializes the model parameters:</strong> Creates the <code>args</code> object that contains the defult values for training our model.</p> <li><p><strong>Loads the train and test data:</strong> Calls the function <code>getdata</code> we defined above.</p> <li><p><strong>Constructs the model:</strong> Builds the model and loads the train and test data sets, and our model onto the GPU &#40;if available&#41;.</p> <li><p><strong>Trains the model:</strong> Defines the <em>callback</em> function <code>evalcb</code> to show the value of the <code>loss_all</code> function during the training process. Then, it sets <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM">ADAM</a> as the optimiser for training out model. Finally, it runs the training process with the macro <code>@epochs</code> for <code>10</code> epochs &#40;as defined in the <code>args</code> object&#41; and shows the <code>accuracy</code> value for the train and test data.</p> </ul> <p>To see the full version of this example, see <a href="https://github.com/FluxML/model-zoo/blob/master/vision/mlp_mnist/mlp_mnist.jl">Simple multi-layer perceptron - model-zoo</a>.</p> <h2 id=resources ><a href="#resources" class=header-anchor >Resources</a></h2> <ul> <li><p><a href="https://www.youtube.com/watch?v&#61;aircAruvnKk&amp;list&#61;PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural networks videos</a>.</p> <li><p><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a>.</p> </ul> <p class=author >– Adarsh Kumar, Mike J Innes, Andrew Dinhobl, Jerry Ling, natema, Zhang Shitian, Liliana Badillo, Dhairya Gandhi</p> </div> </div> </div> <div class="container footer lighter"> <p>Flux: A Deep Learning Library for the Julia Programming Language </p> <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a> <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script> </div> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script src="/fluxml.github.io//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>