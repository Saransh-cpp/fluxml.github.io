<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content=fluxml.ai > <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="../../css/script_default.css"> <link rel=stylesheet  href="../../css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"> </script> <title>Flux â€“ Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="../../"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="../../getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="../../blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="../../tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/dev/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="../../gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=content > <div class=container > <h1>Deep Convolutional Generative Adversarial Network (DCGAN)</h1> <div class=franklin-content > <p>This is a beginner level tutorial for generating images of handwritten digits using a <a href="https://arxiv.org/pdf/1511.06434.pdf">Deep Convolutional Generative Adversarial Network</a> inspired by the <a href="https://www.tensorflow.org/tutorials/generative/dcgan">TensorFlow tutorial on DCGAN</a>.</p> <h2 id=what_are_gans ><a href="#what_are_gans" class=header-anchor >What are GANs?</a></h2> <p><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Neural Networks or simply GANs</a> introduced by Goodfellow et al. is one of the most innovative ideas in modern-day machine learning. GANs are used extensively in the field of image and audio processing to generate high-quality synthetic data that can easily be passed off as real data.</p> <p>A GAN is composed of two sub-models - the <strong>generator</strong> and the <strong>discriminator</strong> acting against one another. The generator can be considered as an artist who draws &#40;generates&#41; new images that look real, whereas the discriminator is a critic who learns to tell real images apart from fakes.</p> <p><img src="../../assets/tutorialposts/2021-10-8-dcgan-mnist/cat_gan.png" alt="" /></p> <p>The GAN starts with a generator and discriminator which have very little or no idea about the underlying data. During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.</p> <p><img src="https://www.tensorflow.org/tutorials/generative/images/gan2.png" alt="" /></p> <p><a href="https://www.tensorflow.org/tutorials/generative/dcgan">&#91;source&#93;</a></p> <p>This tutorial demonstrates the process of training a DC-GAN on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset for handwritten digits</a>. The following animation shows a series of images produced by the generator as it was trained for 25 epochs. The images begin as random noise, but over time, the images become increasingly similar to handwritten numbers.</p> <br><br> <p align=center > <img src="../../assets/tutorialposts/2021-10-8-dcgan-mnist/output.gif" align=middle  width=200 > </p> <h2 id=setup ><a href="#setup" class=header-anchor >Setup</a></h2> <p>We need to install some Julia packages before we start with our implementation of DCGAN.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Pkg

<span class=hljs-comment ># Activate a new project environment in the current directory</span>
Pkg.activate(<span class=hljs-string >&quot;.&quot;</span>)
<span class=hljs-comment ># Add the required packages to the environment</span>
Pkg.add([<span class=hljs-string >&quot;Images&quot;</span>, <span class=hljs-string >&quot;Flux&quot;</span>, <span class=hljs-string >&quot;MLDatasets&quot;</span>, <span class=hljs-string >&quot;CUDA&quot;</span>, <span class=hljs-string >&quot;Parameters&quot;</span>])</code></pre> <p><em>Note: Depending on your internet speed, it may take a few minutes for the packages install.</em></p> <p>&lt;br&gt; After installing the libraries, load the required packages and functions:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Base.Iterators: partition
<span class=hljs-keyword >using</span> Printf
<span class=hljs-keyword >using</span> Statistics
<span class=hljs-keyword >using</span> Random
<span class=hljs-keyword >using</span> Images
<span class=hljs-keyword >using</span> Parameters: <span class=hljs-meta >@with_kw</span>
<span class=hljs-keyword >using</span> Flux
<span class=hljs-keyword >using</span> Flux.Data: DataLoader
<span class=hljs-keyword >using</span> Flux.Optimise: update!
<span class=hljs-keyword >using</span> Flux.Losses: logitbinarycrossentropy
<span class=hljs-keyword >using</span> MLDatasets: MNIST
<span class=hljs-keyword >using</span> CUDA</code></pre> <p>&lt;br&gt; Now we set default values for the learning rates, batch size, epochs, the usage of a GPU &#40;if available&#41; and other hyperparameters for our model.</p> <pre><code class="julia hljs"><span class=hljs-meta >@with_kw</span> <span class=hljs-keyword >struct</span> HyperParams
    batch_size::<span class=hljs-built_in >Int</span> = <span class=hljs-number >128</span>
    latent_dim::<span class=hljs-built_in >Int</span> = <span class=hljs-number >100</span>
    epochs::<span class=hljs-built_in >Int</span> = <span class=hljs-number >25</span>
    verbose_freq::<span class=hljs-built_in >Int</span> = <span class=hljs-number >1000</span>
    output_dim::<span class=hljs-built_in >Int</span> = <span class=hljs-number >5</span>
    disc_lr::<span class=hljs-built_in >Float64</span> = <span class=hljs-number >0.0002</span>
    gen_lr::<span class=hljs-built_in >Float64</span> = <span class=hljs-number >0.0002</span>
    device::<span class=hljs-built_in >Function</span> = gpu
<span class=hljs-keyword >end</span></code></pre> <h2 id=loading_the_data ><a href="#loading_the_data" class=header-anchor >Loading the data</a></h2> <p>As mentioned before, we will be using the MNIST dataset for handwritten digits. So we begin with a simple function for loading and pre-processing the MNIST images:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> load_MNIST_images(hparams)
    images = MNIST.traintensor(<span class=hljs-built_in >Float32</span>)

    <span class=hljs-comment ># Normalize the images to (-1, 1)</span>
    normalized_images = @. <span class=hljs-number >2f0</span> * images - <span class=hljs-number >1f0</span>
    image_tensor = reshape(normalized_images, <span class=hljs-number >28</span>, <span class=hljs-number >28</span>, <span class=hljs-number >1</span>, :)

    <span class=hljs-comment ># Create a dataloader that iterates over mini-batches of the image tensor</span>
    dataloader = DataLoader(image_tensor, batchsize=hparams.batch_size, shuffle=<span class=hljs-literal >true</span>)

    <span class=hljs-keyword >return</span> dataloader
<span class=hljs-keyword >end</span></code></pre> <p>To learn more about loading images in Flux, you can check out <a href="https://fluxml.ai/tutorials/2021/01/21/data-loader.html">this tutorial</a>.</p> <p><em>Note: The data returned from the dataloader is loaded is on the CPU. To train on the GPU, we need to transfer the data to the GPU beforehand.</em></p> <h2 id=create_the_models ><a href="#create_the_models" class=header-anchor >Create the models</a></h2> <h3 id=the_generator ><a href="#the_generator" class=header-anchor >The generator</a></h3> <p>Our generator, a.k.a. the artist, is a neural network that maps low dimensional data to a high dimensional form.</p> <ul> <li><p>This low dimensional data &#40;seed&#41; is generally a vector of random values sampled from a normal distribution.</p> <li><p>The high dimensional data is the generated image.</p> </ul> <p>The <code>Dense</code> layer is used for taking the seed as an input which is upsampled several times using the <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.ConvTranspose">ConvTranspose</a> layer until we reach the desired output size &#40;in our case, 28x28x1&#41;. Furthermore, after each <code>ConvTranspose</code> layer, we apply the Batch Normalization to stabilize the learning process.</p> <p>We will be using the <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.relu">relu</a> activation function for each layer except the output layer, where we use <code>tanh</code> activation.</p> <p>We will also apply the weight initialization method mentioned in the original DCGAN paper.</p> <pre><code class="julia hljs"><span class=hljs-comment ># Function for intializing the model weights with values </span>
<span class=hljs-comment ># sampled from a Gaussian distribution with Î¼=0 and Ïƒ=0.02</span>
dcgan_init(shape...) = randn(<span class=hljs-built_in >Float32</span>, shape) * <span class=hljs-number >0.02f0</span></code></pre> <p>&lt;br&gt;</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> Generator(latent_dim)
    Chain(
        Dense(latent_dim, <span class=hljs-number >7</span>*<span class=hljs-number >7</span>*<span class=hljs-number >256</span>, bias=<span class=hljs-literal >false</span>),
        BatchNorm(<span class=hljs-number >7</span>*<span class=hljs-number >7</span>*<span class=hljs-number >256</span>, relu),

        x -&gt; reshape(x, <span class=hljs-number >7</span>, <span class=hljs-number >7</span>, <span class=hljs-number >256</span>, :),

        ConvTranspose((<span class=hljs-number >5</span>, <span class=hljs-number >5</span>), <span class=hljs-number >256</span> =&gt; <span class=hljs-number >128</span>; stride = <span class=hljs-number >1</span>, pad = <span class=hljs-number >2</span>, init = dcgan_init, bias=<span class=hljs-literal >false</span>),
        BatchNorm(<span class=hljs-number >128</span>, relu),

        ConvTranspose((<span class=hljs-number >4</span>, <span class=hljs-number >4</span>), <span class=hljs-number >128</span> =&gt; <span class=hljs-number >64</span>; stride = <span class=hljs-number >2</span>, pad = <span class=hljs-number >1</span>, init = dcgan_init, bias=<span class=hljs-literal >false</span>),
        BatchNorm(<span class=hljs-number >64</span>, relu),

        <span class=hljs-comment ># The tanh activation ensures that output is in range of (-1, 1)</span>
        ConvTranspose((<span class=hljs-number >4</span>, <span class=hljs-number >4</span>), <span class=hljs-number >64</span> =&gt; <span class=hljs-number >1</span>, tanh; stride = <span class=hljs-number >2</span>, pad = <span class=hljs-number >1</span>, init = dcgan_init, bias=<span class=hljs-literal >false</span>),
    )
<span class=hljs-keyword >end</span></code></pre> <p>&lt;br&gt; Time for a small test&#33;&#33; We create a dummy generator and feed a random vector as a seed to the generator. If our generator is initialized correctly it will return an array of size &#40;28, 28, 1, <code>batch_size</code>&#41;. The <code>@assert</code> macro in Julia will raise an exception for the wrong output size.</p> <pre><code class="julia hljs"><span class=hljs-comment ># Create a dummy generator of latent dim 100</span>
generator = Generator(<span class=hljs-number >100</span>)
noise = randn(<span class=hljs-built_in >Float32</span>, <span class=hljs-number >100</span>, <span class=hljs-number >3</span>) <span class=hljs-comment ># The last axis is the batch size</span>

<span class=hljs-comment ># Feed the random noise to the generator</span>
gen_image = generator(noise)
<span class=hljs-meta >@assert</span> size(gen_image) == (<span class=hljs-number >28</span>, <span class=hljs-number >28</span>, <span class=hljs-number >1</span>, <span class=hljs-number >3</span>)</code></pre> <p>&lt;br&gt; Our generator model is yet to learn the correct weights, so it does not produce a recognizable image for now. To train our poor generator we need its equal rival, the <em>discriminator</em>. &lt;br&gt; &lt;br&gt;</p> <h3 id=discriminator ><a href="#discriminator" class=header-anchor >Discriminator</a></h3> <p>The Discriminator is a simple CNN based image classifier. The <code>Conv</code> layer a is used with a <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.leakyrelu">leakyrelu</a> activation function. </p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> Discriminator()
    Chain(
        Conv((<span class=hljs-number >4</span>, <span class=hljs-number >4</span>), <span class=hljs-number >1</span> =&gt; <span class=hljs-number >64</span>; stride = <span class=hljs-number >2</span>, pad = <span class=hljs-number >1</span>, init = dcgan_init),
        x-&gt;leakyrelu.(x, <span class=hljs-number >0.2f0</span>),
        Dropout(<span class=hljs-number >0.3</span>),

        Conv((<span class=hljs-number >4</span>, <span class=hljs-number >4</span>), <span class=hljs-number >64</span> =&gt; <span class=hljs-number >128</span>; stride = <span class=hljs-number >2</span>, pad = <span class=hljs-number >1</span>, init = dcgan_init),
        x-&gt;leakyrelu.(x, <span class=hljs-number >0.2f0</span>),
        Dropout(<span class=hljs-number >0.3</span>),

        <span class=hljs-comment ># The output is now of the shape (7, 7, 128, batch_size)</span>
        flatten,
        Dense(<span class=hljs-number >7</span> * <span class=hljs-number >7</span> * <span class=hljs-number >128</span>, <span class=hljs-number >1</span>) 
    )
<span class=hljs-keyword >end</span></code></pre> <p>For a more detailed implementation of a CNN-based image classifier, you can refer to <a href="https://fluxml.ai/tutorials/2021/02/07/convnet.html">this tutorial</a>.</p> <p>Now let us check if our discriminator is working:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Dummy Discriminator</span>
discriminator = Discriminator()
<span class=hljs-comment ># We pass the generated image to the discriminator</span>
logits = discriminator(gen_image)
<span class=hljs-meta >@assert</span> size(logits) == (<span class=hljs-number >1</span>, <span class=hljs-number >3</span>)</code></pre> <p>&lt;br&gt; Just like our dummy generator, the untrained discriminator has no idea about what is a real or fake image. It needs to be trained alongside the generator to output positive values for real images, and negative values for fake images.</p> <h2 id=loss_functions_for_gan ><a href="#loss_functions_for_gan" class=header-anchor >Loss functions for GAN</a></h2> <p>In a GAN problem, there are only two labels involved: fake and real. So Binary CrossEntropy is an easy choice for a preliminary loss function. </p> <p>But even if Flux&#39;s <code>binarycrossentropy</code> does the job for us, due to numerical stability it is always preferred to compute cross-entropy using logits. Flux provides <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.logitbinarycrossentropy">logitbinarycrossentropy</a> specifically for this purpose. Mathematically it is equivalent to <code>binarycrossentropy&#40;Ïƒ&#40;Å·&#41;, y, kwargs...&#41;.</code> &lt;br&gt;</p> <h3 id=discriminator_loss ><a href="#discriminator_loss" class=header-anchor >Discriminator Loss</a></h3> <p>The discriminator loss quantifies how well the discriminator can distinguish real images from fakes. It compares </p> <ul> <li><p>discriminator&#39;s predictions on real images to an array of 1s, and</p> <li><p>discriminator&#39;s predictions on fake &#40;generated&#41; images to an array of 0s.</p> </ul> <p>These two losses are summed together to give a scalar loss. So we can write the loss function of the discriminator as:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> discriminator_loss(real_output, fake_output)
    real_loss = logitbinarycrossentropy(real_output, <span class=hljs-number >1</span>)
    fake_loss = logitbinarycrossentropy(fake_output, <span class=hljs-number >0</span>)
    <span class=hljs-keyword >return</span> real_loss + fake_loss
<span class=hljs-keyword >end</span></code></pre> <p>&lt;br&gt;</p> <h3 id=generator_loss ><a href="#generator_loss" class=header-anchor >Generator Loss</a></h3> <p>The generator&#39;s loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real &#40;or 1&#41;.</p> <pre><code class="julia hljs">generator_loss(fake_output) = logitbinarycrossentropy(fake_output, <span class=hljs-number >1</span>)</code></pre>
<p>&lt;br&gt; We also need optimizers for our network. Why you may ask? Read more <a href="https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5">here</a>. For both the generator and discriminator, we will use the <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM">ADAM optimizer</a>.</p>
<h2 id=utility_functions ><a href="#utility_functions" class=header-anchor >Utility functions</a></h2>
<p>The output of the generator ranges from &#40;-1, 1&#41;, so it needs to be de-normalized before we can display it as an image. To make things a bit easier, we define a function to visualize the output of the generator as a grid of images. </p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> create_output_image(gen, fixed_noise, hparams)
    fake_images = cpu(gen.(fixed_noise))
    image_array = reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_dim)))
    image_array = permutedims(dropdims(image_array; dims=(<span class=hljs-number >3</span>, <span class=hljs-number >4</span>)), (<span class=hljs-number >2</span>, <span class=hljs-number >1</span>))
    image_array = @. Gray(image_array + <span class=hljs-number >1f0</span>) / <span class=hljs-number >2f0</span>
    <span class=hljs-keyword >return</span> image_array
<span class=hljs-keyword >end</span></code></pre>
<h2 id=training ><a href="#training" class=header-anchor >Training</a></h2>
<p>For the sake of simplifying our training problem, we will divide the generator and discriminator training into two separate functions. </p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> train_discriminator!(gen, disc, real_img, fake_img, opt, ps, hparams)

    disc_loss, grads = Flux.withgradient(ps) <span class=hljs-keyword >do</span>
        discriminator_loss(disc(real_img), disc(fake_img))
    <span class=hljs-keyword >end</span>

    <span class=hljs-comment ># Update the discriminator parameters</span>
    update!(opt, ps, grads)
    <span class=hljs-keyword >return</span> disc_loss
<span class=hljs-keyword >end</span></code></pre>
<p>&lt;br&gt; We define a similar function for the generator.</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> train_generator!(gen, disc, fake_img, opt, ps, hparams)

    gen_loss, grads = Flux.withgradient(ps) <span class=hljs-keyword >do</span>
        generator_loss(disc(fake_img))
    <span class=hljs-keyword >end</span>

    update!(opt, ps, grads)
    <span class=hljs-keyword >return</span> gen_loss
<span class=hljs-keyword >end</span></code></pre>
<p>&lt;br&gt;</p>
<p>Now that we have defined every function we need, we integrate everything into a single <code>train</code> function where we first set up all the models and optimizers and then train the GAN for a specified number of epochs.</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> train(hparams)

    dev = hparams.device
    <span class=hljs-comment ># Check if CUDA is actually present</span>
    <span class=hljs-keyword >if</span> hparams.device == gpu
        <span class=hljs-keyword >if</span> !CUDA.has_cuda()
        dev = cpu
        <span class=hljs-meta >@warn</span> <span class=hljs-string >&quot;No gpu found, falling back to CPU&quot;</span>
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>

    <span class=hljs-comment ># Load the normalized MNIST images</span>
    dataloader = load_MNIST_images(hparams)

    <span class=hljs-comment ># Initialize the models and pass them to correct device</span>
    disc = Discriminator() |&gt; dev
    gen =  Generator(hparams.latent_dim) |&gt; dev

    <span class=hljs-comment ># Collect the generator and discriminator parameters</span>
    disc_ps = params(disc)
    gen_ps = params(gen)

    <span class=hljs-comment ># Initialize the ADAM optimizers for both the sub-models</span>
    <span class=hljs-comment ># with respective learning rates</span>
    disc_opt = ADAM(hparams.disc_lr)
    gen_opt = ADAM(hparams.gen_lr)

    <span class=hljs-comment ># Create a batch of fixed noise for visualizing the training of generator over time</span>
    fixed_noise = [randn(<span class=hljs-built_in >Float32</span>, hparams.latent_dim, <span class=hljs-number >1</span>) |&gt; dev <span class=hljs-keyword >for</span> _=<span class=hljs-number >1</span>:hparams.output_dim^<span class=hljs-number >2</span>]

    <span class=hljs-comment ># Training loop</span>
    train_steps = <span class=hljs-number >0</span>
    <span class=hljs-keyword >for</span> ep <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:hparams.epochs
        <span class=hljs-meta >@info</span> <span class=hljs-string >&quot;Epoch <span class=hljs-variable >$ep</span>&quot;</span>
        <span class=hljs-keyword >for</span> real_img <span class=hljs-keyword >in</span> dataloader

            <span class=hljs-comment ># Transfer the data to the GPU</span>
            real_img = real_img |&gt; dev

            <span class=hljs-comment ># Create a random noise</span>
            noise = randn!(similar(real_img, (hparams.latent_dim, hparams.batch_size)))
            <span class=hljs-comment ># Pass the noise to the generator to create a fake imagae</span>
            fake_img = gen(noise)

            <span class=hljs-comment ># Update discriminator and generator</span>
            loss_disc = train_discriminator!(gen, disc, real_img, fake_img, disc_opt, disc_ps, hparams)
            loss_gen = train_generator!(gen, disc, fake_img, gen_opt, gen_ps, hparams)

            <span class=hljs-keyword >if</span> train_steps % hparams.verbose_freq == <span class=hljs-number >0</span>
                <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot;Train step <span class=hljs-subst >$(train_steps)</span>, Discriminator loss = <span class=hljs-subst >$(loss_disc)</span>, Generator loss = <span class=hljs-subst >$(loss_gen)</span>&quot;</span>)
                <span class=hljs-comment ># Save generated fake image</span>
                output_image = create_output_image(gen, fixed_noise, hparams)
                save(<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;output/dcgan_steps_%06d.png&quot;</span>, train_steps), output_image)
            <span class=hljs-keyword >end</span>
            train_steps += <span class=hljs-number >1</span>
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>

    output_image = create_output_image(gen, fixed_noise, hparams)
    save(<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;output/dcgan_steps_%06d.png&quot;</span>, train_steps), output_image)

    <span class=hljs-keyword >return</span> <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span></code></pre>
<p>&lt;br&gt; Now we finally get to train the GAN:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Define the hyper-parameters (here, we go with the default ones)</span>
hparams = HyperParams()
train(hparams)</code></pre>
<h2 id=output ><a href="#output" class=header-anchor >Output</a></h2>
<p>The generated images are stored inside the <code>output</code> folder. To visualize the output of the generator over time, we create a gif of the generated images.</p>
<pre><code class="julia hljs">folder = <span class=hljs-string >&quot;output&quot;</span>
<span class=hljs-comment ># Get the image filenames from the folder</span>
img_paths = readdir(folder, join=<span class=hljs-literal >true</span>)
<span class=hljs-comment ># Load all the images as an array</span>
images = load.(img_paths)
<span class=hljs-comment ># Join all the images in the array to create a matrix of images</span>
gif_mat = cat(images..., dims=<span class=hljs-number >3</span>)
save(<span class=hljs-string >&quot;./output.gif&quot;</span>, gif_mat)</code></pre>
<p>&lt;br&gt; &lt;p align&#61;&quot;center&quot;&gt; &lt;img src&#61;&quot;../../assets/tutorialposts/2021-10-8-dcgan-mnist/output.gif&quot; align&#61;&quot;middle&quot; width&#61;&quot;200&quot;&gt; &lt;/p&gt;</p>
<h2 id=resources_references ><a href="#resources_references" class=header-anchor >Resources &amp; References</a></h2>
<ul>
<li><p><a href="http&#61;s://github.com/FluxML/model-zoo/blob/master/vision/dcgan_mnist/dcgan_mnist.jl">The DCGAN implementation in Model Zoo.</a></p>

</ul>



<p class=author >â€“ Deeptendu Santra</p>

</div>    

    
      </div>
    </div>
    

    
    <div class="container footer lighter">
      <p>Flux: A Deep Learning Library for the Julia Programming Language
</p>
      <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a>
      <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script>
    </div>

    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    
    
    
    <script src="/fluxml.github.io//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>