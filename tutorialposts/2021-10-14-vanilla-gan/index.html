<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content=fluxml.ai > <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="../../css/script_default.css"> <link rel=stylesheet  href="../../css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"> </script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="../../"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="../../getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="../../blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="../../tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/dev/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="../../gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=content > <div class=container > <h1>Generative Adversarial Networks</h1> <div class=franklin-content > <p>This tutorial describes how to implement a vanilla Generative Adversarial Network using Flux and how train it on the MNIST dataset. It is based on this <a href="https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f">Pytorch tutorial</a>. The original GAN <a href="https://arxiv.org/abs/1406.2661">paper</a> by Goodfellow et al. is a great resource that describes the motivation and theory behind GANs:</p> <pre><code class="julia hljs">In the proposed adversarial nets framework, the generative model is pitted against an adversary: a
discriminative model that learns to determine whether a sample is from the model distribution or the
data distribution. The generative model can be thought of as analogous to a team of counterfeiters,
trying to produce fake currency and use it without detection, <span class=hljs-keyword >while</span> the discriminative model is
analogous to the police, trying to detect the counterfeit currency. Competition <span class=hljs-keyword >in</span> this game drives
both teams to improve their methods until the counterfeits are indistinguishable from the genuine
articles.</code></pre> <p>Let&#39;s implement a GAN in Flux. To get started we first import a few useful packages:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> MLDatasets: MNIST
<span class=hljs-keyword >using</span> Flux.Data: DataLoader
<span class=hljs-keyword >using</span> Flux
<span class=hljs-keyword >using</span> CUDA
<span class=hljs-keyword >using</span> Zygote
<span class=hljs-keyword >using</span> UnicodePlots</code></pre> <p>To download a package in the Julia REPL, type <code>&#93;</code> to enter package mode and then type <code>add MLDatasets</code> or perform this operation with the Pkg module like this</p> <pre><code class="julia hljs">&gt; <span class=hljs-keyword >import</span> Pkg
&gt; Pkg.add(MLDatasets)</code></pre> <p>While <a href="">UnicodePlots</a> is not necessary, it can be used to plot generated samples into the terminal during training. Having direct feedback, instead of looking at plots in a separate window, use fantastic for debugging.</p> <p>Next, let us define values for learning rate, batch size, epochs, and other hyper-parameters. While we are at it, we also define optimizers for the generator and discriminator network. More on what these are later.</p> <pre><code class="julia hljs">lr_g = <span class=hljs-number >2e-4</span>          <span class=hljs-comment ># Learning rate of the generator network</span>
    lr_d = <span class=hljs-number >2e-4</span>          <span class=hljs-comment ># Learning rate of the discriminator network</span>
    batch_size = <span class=hljs-number >128</span>    <span class=hljs-comment ># batch size</span>
    num_epochs = <span class=hljs-number >1000</span>   <span class=hljs-comment ># Number of epochs to train for</span>
    output_period = <span class=hljs-number >100</span> <span class=hljs-comment ># Period length for plots of generator samples</span>
    n_features = <span class=hljs-number >28</span> * <span class=hljs-number >28</span><span class=hljs-comment ># Number of pixels in each sample of the MNIST dataset</span>
    latent_dim = <span class=hljs-number >100</span>    <span class=hljs-comment ># Dimension of latent space</span>
    opt_dscr = ADAM(lr_d)<span class=hljs-comment ># Optimizer for the discriminator</span>
    opt_gen = ADAM(lr_g) <span class=hljs-comment ># Optimizer for the generator</span></code></pre> <p>In this tutorial I&#39;m assuming that a CUDA-enabled GPU is available on the system where the script is running. If this is not the case, simply remove the <code>|&gt;gpu</code> decorators: <a href="https://docs.julialang.org/en/v1/manual/functions/#Function-composition-and-piping">piping</a>.</p> <h2 id=data_loading ><a href="#data_loading" class=header-anchor >Data loading</a></h2> <p>The MNIST data set is available from <a href="https://juliaml.github.io/MLDatasets.jl/latest/">MLDatasets</a>. The first time you instantiate it you will be prompted if you want to download it. You should agree to this. </p> <p>GANs can be trained unsupervised. Therefore only keep the images from the training set and discard the labels.</p> <p>After we load the training data we re-scale the data from values in &#91;0:1&#93; to values in &#91;-1:1&#93;. GANs are notoriously tricky to train and this re-scaling is a recommended <a href="https://github.com/soumith/ganhacks">GAN hack</a>. The re-scaled data is used to define a data loader which handles batching and shuffling the data.</p> <pre><code class="julia hljs"><span class=hljs-comment ># Load the dataset</span>
    train_x, _ = MNIST.traindata(<span class=hljs-built_in >Float32</span>);
    <span class=hljs-comment ># This dataset has pixel values ∈ [0:1]. Map these to [-1:1]</span>
    train_x = <span class=hljs-number >2f0</span> * reshape(train_x, <span class=hljs-number >28</span>, <span class=hljs-number >28</span>, <span class=hljs-number >1</span>, :) .- <span class=hljs-number >1f0</span> |&gt;gpu;
    <span class=hljs-comment ># DataLoader allows to access data batch-wise and handles shuffling.</span>
    train_loader = DataLoader(train_x, batchsize=batch_size, shuffle=<span class=hljs-literal >true</span>);</code></pre> <h2 id=defining_the_networks ><a href="#defining_the_networks" class=header-anchor >Defining the Networks</a></h2> <p>A vanilla GAN, the discriminator and the generator are both plain, <a href="https://boostedml.com/2020/04/feedforward-neural-networks-and-multilayer-perceptrons.html">feed-forward multilayer perceptrons</a>. We use leaky rectified linear units <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.leakyrelu">leakyrelu</a> to ensure out model is non-linear. </p> <p>Here, the coefficient <code>α</code> &#40;in the <code>leakyrelu</code> below&#41;, is set to 0.2. Empirically, this value allows for good training of the network &#40;based on prior experiments&#41;. It has also been found that Dropout ensures a good generalization of the learned network, so we will use that below. Dropout is usually active when training a model and inactive in inference. Flux automatically sets the training mode when calling the model in a gradient context. As a final non-linearity, we use the <code>sigmoid</code> activation function.</p> <pre><code class="julia hljs">discriminator = Chain(Dense(n_features, <span class=hljs-number >1024</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                        Dropout(<span class=hljs-number >0.3</span>),
                        Dense(<span class=hljs-number >1024</span>, <span class=hljs-number >512</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                        Dropout(<span class=hljs-number >0.3</span>),
                        Dense(<span class=hljs-number >512</span>, <span class=hljs-number >256</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                        Dropout(<span class=hljs-number >0.3</span>),
                        Dense(<span class=hljs-number >256</span>, <span class=hljs-number >1</span>, sigmoid)) |&gt; gpu</code></pre> <p>Let&#39;s define the generator in a similar fashion. This network maps a latent variable &#40;a variable that is not directly observed but instead inferred&#41; to the image space and we set the input and output dimension accordingly. A <code>tanh</code> squashes the output of the final layer to values in &#91;-1:1&#93;, the same range that we squashed the training data onto.</p> <pre><code class="julia hljs">generator = Chain(Dense(latent_dim, <span class=hljs-number >256</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                    Dense(<span class=hljs-number >256</span>, <span class=hljs-number >512</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                    Dense(<span class=hljs-number >512</span>, <span class=hljs-number >1024</span>, x -&gt; leakyrelu(x, <span class=hljs-number >0.2f0</span>)),
                    Dense(<span class=hljs-number >1024</span>, n_features, tanh)) |&gt; gpu</code></pre> <h2 id=training_functions_for_the_networks ><a href="#training_functions_for_the_networks" class=header-anchor >Training functions for the networks</a></h2> <p>To train the discriminator, we present it with real data from the MNIST data set and with fake data and reward it by predicting the correct labels for each sample. The correct labels are of course 1 for in-distribution data and 0 for out-of-distribution data coming from the generator. <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.binarycrossentropy">Binary cross entropy</a> is the loss function of choice. While the Flux documentation suggests to use <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.logitcrossentropy">Logit binary cross entropy</a>, the GAN seems to be difficult to train with this loss function. This function returns the discriminator loss for logging purposes. We can calculate the loss in the same call as evaluating the pullback and resort to getting the pullback directly from Zygote instead of calling <code>Flux.train&#33;</code> on the model. To calculate the gradients of the loss function with respect to the parameters of the discriminator we then only have to evaluate the pullback with a seed gradient of 1.0. These gradients are used to update the model parameters</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> train_dscr!(discriminator, real_data, fake_data)
    this_batch = size(real_data)[<span class=hljs-keyword >end</span>] <span class=hljs-comment ># Number of samples in the batch</span>
    <span class=hljs-comment ># Concatenate real and fake data into one big vector</span>
    all_data = hcat(real_data, fake_data)

    <span class=hljs-comment ># Target vector for predictions: 1 for real data, 0 for fake data.</span>
    all_target = [ones(eltype(real_data), <span class=hljs-number >1</span>, this_batch) zeros(eltype(fake_data), <span class=hljs-number >1</span>, this_batch)] |&gt; gpu;

    ps = Flux.params(discriminator)
    loss, pullback = Zygote.pullback(ps) <span class=hljs-keyword >do</span>
        preds = discriminator(all_data)
        loss = Flux.Losses.binarycrossentropy(preds, all_target)
    <span class=hljs-keyword >end</span>
    <span class=hljs-comment ># To get the gradients we evaluate the pullback with 1.0 as a seed gradient.</span>
    grads = pullback(<span class=hljs-number >1f0</span>)

    <span class=hljs-comment ># Update the parameters of the discriminator with the gradients we calculated above</span>
    Flux.update!(opt_dscr, Flux.params(discriminator), grads)
    
    <span class=hljs-keyword >return</span> loss 
<span class=hljs-keyword >end</span></code></pre> <p>Now we need to define a function to train the generator network. The job of the generator is to fool the discriminator so we reward the generator when the discriminator predicts a high probability for its samples to be real data. In the training function we first need to sample some noise, i.e. normally distributed data. This has to be done outside the pullback since we don&#39;t want to get the gradients with respect to the noise, but to the generator parameters. Inside the pullback we need to first apply the generator to the noise since we will take the gradient with respect to the parameters of the generator. We also need to call the discriminator in order to evaluate the loss function inside the pullback. Here we need to remember to deactivate the dropout layers of the discriminator. We do this by setting the discriminator into test mode before the pullback. Immediately after the pullback we set it back into training mode. Then we evaluate the pullback, call it with a seed gradient of 1.0 as above, update the parameters of the generator network and return the loss.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> train_gen!(discriminator, generator)
    <span class=hljs-comment ># Sample noise</span>
    noise = randn(latent_dim, batch_size) |&gt; gpu;

    <span class=hljs-comment ># Define parameters and get the pullback</span>
    ps = Flux.params(generator)
    <span class=hljs-comment ># Set discriminator into test mode to disable dropout layers</span>
    testmode!(discriminator)
    <span class=hljs-comment ># Evaluate the loss function while calculating the pullback. We get the loss for free</span>
    loss, back = Zygote.pullback(ps) <span class=hljs-keyword >do</span>
        preds = discriminator(generator(noise));
        loss = Flux.Losses.binarycrossentropy(preds, <span class=hljs-number >1.</span>) 
    <span class=hljs-keyword >end</span>
    <span class=hljs-comment ># Evaluate the pullback with a seed-gradient of 1.0 to get the gradients for</span>
    <span class=hljs-comment ># the parameters of the generator</span>
    grads = back(<span class=hljs-number >1.0f0</span>)
    Flux.update!(opt_gen, Flux.params(generator), grads)
    <span class=hljs-comment ># Set discriminator back into automatic mode</span>
    trainmode!(discriminator, mode=:auto)
    <span class=hljs-keyword >return</span> loss
<span class=hljs-keyword >end</span></code></pre> <h2 id=training ><a href="#training" class=header-anchor >Training</a></h2> <p>Now we are ready to train the GAN. In the training loop we keep track of the per-sample loss of the generator and the discriminator, where we use the batch loss returned by the two training functions defined above. In each epoch we iterate over the mini-batches given by the data loader. Only minimal data processing needs to be done before the training functions can be called. </p> <pre><code class="julia hljs">lossvec_gen = zeros(num_epochs)
lossvec_dscr = zeros(num_epochs)

<span class=hljs-keyword >for</span> n <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:num_epochs
    loss_sum_gen = <span class=hljs-number >0.0f0</span>
    loss_sum_dscr = <span class=hljs-number >0.0f0</span>

    <span class=hljs-keyword >for</span> x <span class=hljs-keyword >in</span> train_loader
        <span class=hljs-comment ># - Flatten the images from 28x28xbatchsize to 784xbatchsize</span>
        real_data = flatten(x);

        <span class=hljs-comment ># Train the discriminator</span>
        noise = randn(latent_dim, size(x)[<span class=hljs-keyword >end</span>]) |&gt; gpu
        fake_data = generator(noise)
        loss_dscr = train_dscr!(discriminator, real_data, fake_data)
        loss_sum_dscr += loss_dscr

        <span class=hljs-comment ># Train the generator</span>
        loss_gen = train_gen!(discriminator, generator)
        loss_sum_gen += loss_gen
    <span class=hljs-keyword >end</span>

    <span class=hljs-comment ># Add the per-sample loss of the generator and discriminator</span>
    lossvec_gen[n] = loss_sum_gen / size(train_x)[<span class=hljs-keyword >end</span>]
    lossvec_dscr[n] = loss_sum_dscr / size(train_x)[<span class=hljs-keyword >end</span>]

    <span class=hljs-keyword >if</span> n % output_period == <span class=hljs-number >0</span>
        <span class=hljs-meta >@show</span> n
        noise = randn(latent_dim, <span class=hljs-number >4</span>) |&gt; gpu;
        fake_data = reshape(generator(noise), <span class=hljs-number >28</span>, <span class=hljs-number >4</span>*<span class=hljs-number >28</span>);
        p = heatmap(fake_data, colormap=:inferno)
        print(p)
    <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>For the hyper-parameters shown in this example, the generator produces useful images after about 1000 epochs. And after about 5000 epochs the result look indistinguishable from real MNIST data. Using a Nvidia V100 GPU on a 2.7 GHz Power9 CPU with 32 hardware threads, training 100 epochs takes about 80 seconds when using the GPU. The GPU utilization is between 30 and 40&#37;. To observe the network more frequently during training you can for example set <code>output_period&#61;20</code>. Training the GAN using the CPU takes about 10 minutes per epoch and is not recommended.</p> <h2 id=results ><a href="#results" class=header-anchor >Results</a></h2> <p>Below you can see what some of the images output may look like after different numbers of epochs.</p> <p><img src="https://user-images.githubusercontent.com/35577566/138465727-3729b867-2c2c-4f12-ba8e-e7b00c73d82c.png" alt="" /></p> <p><img src="https://user-images.githubusercontent.com/35577566/138465750-423f70fc-c8e7-489c-8cf4-f01b203a24dd.png" alt="" /></p> <p><img src="https://user-images.githubusercontent.com/35577566/138465777-5c8252ae-e43b-4708-a42a-b0b85324f79d.png" alt="" /></p> <p><img src="https://user-images.githubusercontent.com/35577566/138465803-07239e62-9e68-42b7-9bb7-57fdff748ba9.png" alt="" /></p> <h2 id=resources ><a href="#resources" class=header-anchor >Resources</a></h2> <ul> <li><p><a href="https://github.com/AdarshKumar712/FluxGAN">A collection of GANs in Flux</a></p> <li><p><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Wikipedia</a></p> <li><p><a href="https://github.com/soumith/ganhacks">GAN hacks</a></p> </ul> <p class=author >– Ralph Kube</p> </div> </div> </div> <div class="container footer lighter"> <p>Flux: A Deep Learning Library for the Julia Programming Language </p> <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a> <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script> </div> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script> <script src="/.//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>