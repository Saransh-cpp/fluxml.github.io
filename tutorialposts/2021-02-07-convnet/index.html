<!DOCTYPE html> <html lang=en > <style> #cppn{ position:absolute; top:0; bottom:0; height: 100%; width: 100vw; } iframe { display: block; border-style:none; } </style> <meta property="og:title" content=Flux.jl > <meta property="og:description" content="The elegant machine learning library"> <meta property="og:image" content="/assets/images/FluxGitHubPreview.png"> <meta property="og:url" content=fluxml.ai > <meta name="twitter:title" content=Flux.jl > <meta name="twitter:description" content="The elegant machine learning library"> <meta name="twitter:image" content="/assets/images/FluxGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <link rel=apple-touch-icon  sizes=180x180  href="assets/favicon_io/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="assets/favicon_io/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="assets/favicon_io/favicon-16x16.png"> <link rel=manifest  href="assets/favicon_io/site.webmanifest"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-36890222-9'); </script> <meta charset=utf-8 > <meta name=viewport  content="width=device-width, initial-scale=1, shrink-to-fit=no"> <link rel=stylesheet  href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin=anonymous > <link rel=stylesheet  href="../../css/script_default.css"> <link rel=stylesheet  href="../../css/site.css"> <link rel=stylesheet  href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin=anonymous > <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin=anonymous ></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin=anonymous  onload="renderMathInElement(document.body, { delimiters: [ {left: '$[[', right: ']]', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '[[', right: ']]', display: false} ] });"> </script> <title>Flux – Elegant ML</title> <nav class="navbar navbar-expand-lg navbar-dark container lighter"> <a class=navbar-brand  href="../../"> <div class=logo  style="font-size:30pt;margin-top:-15px;margin-bottom:-10px;">flux</div> </a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class=nav-item > <a class=nav-link  href="../../getting_started/">Getting Started</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/" target=_blank >Docs</a> <li class=nav-item > <a class=nav-link  href="../../blog/">Blog</a> <li class=nav-item > <a class=nav-link  href="../../tutorials/">Tutorials</a> <li class=nav-item > <a class=nav-link  href="https://fluxml.ai/Flux.jl/dev/ecosystem/">Ecosystem</a> <li class=nav-item > <a class=nav-link  href="../../gsoc/">GSoC</a> <li class=nav-item > <a class=nav-link  href="https://discourse.julialang.org/c/domain/ML" target=_blank >Discuss</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl" target=_blank >GitHub</a> <li class=nav-item > <a class=nav-link  href="https://stackoverflow.com/questions/tagged/flux.jl" target=_blank >Stack Overflow</a> <li class=nav-item > <a class=nav-link  href="https://github.com/FluxML/Flux.jl/blob/master/CONTRIBUTING.md" target=_blank >Contribute</a> </ul> </div> </nav> <div class=content > <div class=container > <h1>Simple ConvNet</h1> <div class=franklin-content > <p>In this tutorial, we build a simple Convolutional Neural Network &#40;ConvNet&#41; to classify the MNIST dataset. This model has a simple architecture with three feature detection layers &#40;Conv -&gt; ReLU -&gt; MaxPool&#41; followed by a final dense layer that classifies MNIST handwritten digits. Note that this model, while simple, should hit around 99&#37; test accuracy after training for approximately 20 epochs.</p> <p>This example writes out the saved model to the file <code>mnist_conv.bson</code>. Also, it demonstrates basic model construction, training, saving, conditional early-exit, and learning rate scheduling.</p> <p>To run this example, we need the following packages:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Flux, MLDatasets, Statistics
<span class=hljs-keyword >using</span> Flux: onehotbatch, onecold, logitcrossentropy
<span class=hljs-keyword >using</span> MLDatasets: MNIST
<span class=hljs-keyword >using</span> Base.Iterators: partition
<span class=hljs-keyword >using</span> Printf, BSON
<span class=hljs-keyword >using</span> Parameters: <span class=hljs-meta >@with_kw</span>
<span class=hljs-keyword >using</span> CUDA
CUDA.allowscalar(<span class=hljs-literal >false</span>)</code></pre> <p>We set default values for learning rate, batch size, number of epochs, and path for saving the file <code>mnist_conv.bson</code>:</p> <pre><code class="julia hljs"><span class=hljs-meta >@with_kw</span> <span class=hljs-keyword >mutable struct</span> Args
   lr::<span class=hljs-built_in >Float64</span> = <span class=hljs-number >3e-3</span>
   epochs::<span class=hljs-built_in >Int</span> = <span class=hljs-number >20</span>
   batch_size = <span class=hljs-number >128</span>
   savepath::<span class=hljs-built_in >String</span> = <span class=hljs-string >&quot;./&quot;</span>
<span class=hljs-keyword >end</span></code></pre> <p>To train our model, we need to bundle images together with their labels and group them into mini-batches &#40;makes the training process faster&#41;. We define the function <code>make_minibatch</code> that takes as inputs the images &#40;<code>X</code>&#41; and their labels &#40;<code>Y</code>&#41; as well as the indices for the mini-batches &#40;<code>idx</code>&#41;:</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> make_minibatch(X, Y, idxs)
   X_batch = <span class=hljs-built_in >Array</span>{<span class=hljs-built_in >Float32</span>}(<span class=hljs-literal >undef</span>, size(X)[<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>]..., <span class=hljs-number >1</span>, length(idxs))
   <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:length(idxs)
       X_batch[:, :, :, i] = <span class=hljs-built_in >Float32</span>.(X[:,:,idxs[i]])
   <span class=hljs-keyword >end</span>
   Y_batch = onehotbatch(Y[idxs], <span class=hljs-number >0</span>:<span class=hljs-number >9</span>)
   <span class=hljs-keyword >return</span> (X_batch, Y_batch)
<span class=hljs-keyword >end</span></code></pre> <p><code>make_minibatch</code> takes the following steps:</p> <ul> <li><p>Creates the <code>X_batch</code> array of size <code>28x28x1x128</code> to store the mini-batches. </p> <li><p>Stores the mini-batches in <code>X_batch</code>.</p> <li><p>One hot encodes the labels of the images.</p> <li><p>Stores the labels in <code>Y_batch</code>.</p> </ul> <p><code>get_processed_data</code> loads the train and test data from <code>Flux.Data.MNIST</code>. First, it loads the images and labels of the train data set, and creates an array that contains the indices of the train images that correspond to each mini-batch &#40;of size <code>args.batch_size</code>&#41;. Then, it calls the <code>make_minibatch</code> function to create all of the train mini-batches. Finally, it loads the test images and creates one mini-batch that contains them all.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> get_processed_data(args)
   <span class=hljs-comment ># Load labels and images</span>
   train_imgs, train_labels = MNIST.traindata()
   mb_idxs = partition(<span class=hljs-number >1</span>:length(train_labels), args.batch_size)
   train_set = [make_minibatch(train_imgs, train_labels, i) <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> mb_idxs]
  
   <span class=hljs-comment ># Prepare test set as one giant minibatch:</span>
   test_imgs, test_labels = MNIST.testdata()
   test_set = make_minibatch(test_imgs, test_labels, <span class=hljs-number >1</span>:length(test_labels))
 
   <span class=hljs-keyword >return</span> train_set, test_set
 
<span class=hljs-keyword >end</span></code></pre> <p>Now, we define the <code>build_model</code> function that creates a ConvNet model which is composed of <em>three</em> convolution layers &#40;feature detection&#41; and <em>one</em> classification layer. The input layer size is <code>28x28</code>. The images are grayscale, which means there is only <em>one</em> channel &#40;compared to 3 for RGB&#41; in every data point. Combined together, the convolutional layer structure would look like <code>Conv&#40;kernel, input_channels &#61;&gt; output_channels, ...&#41;</code>. Each convolution layer reduces the size of the image by applying the Rectified Linear unit &#40;ReLU&#41; and MaxPool operations. On the other hand, the classification layer outputs a vector of 10 dimensions &#40;a dense layer&#41;, that is, the number of classes that the model will be able to predict.</p> <pre><code class="julia hljs"><span class=hljs-keyword >function</span> build_model(args; imgsize = (<span class=hljs-number >28</span>,<span class=hljs-number >28</span>,<span class=hljs-number >1</span>), nclasses = <span class=hljs-number >10</span>)
   cnn_output_size = <span class=hljs-built_in >Int</span>.(floor.([imgsize[<span class=hljs-number >1</span>]/<span class=hljs-number >8</span>,imgsize[<span class=hljs-number >2</span>]/<span class=hljs-number >8</span>,<span class=hljs-number >32</span>])) 
 
   <span class=hljs-keyword >return</span> Chain(
   <span class=hljs-comment ># First convolution, operating upon a 28x28 image</span>
   Conv((<span class=hljs-number >3</span>, <span class=hljs-number >3</span>), imgsize[<span class=hljs-number >3</span>]=&gt;<span class=hljs-number >16</span>, pad=(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>), relu),
   MaxPool((<span class=hljs-number >2</span>,<span class=hljs-number >2</span>)),
 
   <span class=hljs-comment ># Second convolution, operating upon a 14x14 image</span>
   Conv((<span class=hljs-number >3</span>, <span class=hljs-number >3</span>), <span class=hljs-number >16</span>=&gt;<span class=hljs-number >32</span>, pad=(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>), relu),
   MaxPool((<span class=hljs-number >2</span>,<span class=hljs-number >2</span>)),
 
   <span class=hljs-comment ># Third convolution, operating upon a 7x7 image</span>
   Conv((<span class=hljs-number >3</span>, <span class=hljs-number >3</span>), <span class=hljs-number >32</span>=&gt;<span class=hljs-number >32</span>, pad=(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>), relu),
   MaxPool((<span class=hljs-number >2</span>,<span class=hljs-number >2</span>)),
 
   <span class=hljs-comment ># Reshape 3d array into a 2d one using `Flux.flatten`, at this point it should be (3, 3, 32, N)</span>
   flatten,
   Dense(prod(cnn_output_size), <span class=hljs-number >10</span>))
<span class=hljs-keyword >end</span></code></pre> <p>To chain the layers of a model we use the Flux function <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Chain">Chain</a>. It enables us to call the layers in sequence on a given input. Also, we use the function <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.flatten">flatten</a> to reshape the output image from the last convolution layer. Finally, we call the <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dense">Dense</a> function to create the classification layer.</p> <p>Before training our model, we need to define a few functions that will be helpful for the process:</p> <ul> <li><p><code>augment</code> augments the data by adding gaussian random noise to our image to make it more robust:</p> </ul> <pre><code class="julia hljs">augment(x) = x .+ gpu(<span class=hljs-number >0.1f0</span>*randn(eltype(x), size(x)))</code></pre>
<ul>
<li><p><code>anynan</code> checks whether any element of the params is NaN or not:</p>

</ul>
<pre><code class="julia hljs">anynan(x) = any(y -&gt; any(isnan, y), x)</code></pre>
<ul>
<li><p><code>accuracy</code> computes the accuracy of our ConvNet:</p>

</ul>
<pre><code class="julia hljs">accuracy(x, y, model) = mean(onecold(cpu(model(x))) .== onecold(cpu(y)))</code></pre>
<p>Finally, we define the <code>train</code> function:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> train(; kws...)   
   args = Args(; kws...)
 
   <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot;Loading data set&quot;</span>)
   train_set, test_set = get_processed_data(args)
 
   <span class=hljs-comment ># Define our model.  We will use a simple convolutional architecture with</span>
   <span class=hljs-comment ># three iterations of Conv -&gt; ReLU -&gt; MaxPool, followed by a final Dense layer.</span>
   <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot;Building model...&quot;</span>)
   model = build_model(args)
 
   <span class=hljs-comment ># Load model and datasets onto GPU, if enabled</span>
   train_set = gpu.(train_set)
   test_set = gpu.(test_set)
   model = gpu(model)
  
   <span class=hljs-comment ># Make sure our model is nicely precompiled before starting our training loop</span>
   model(train_set[<span class=hljs-number >1</span>][<span class=hljs-number >1</span>])
 
   <span class=hljs-comment ># `loss()` calculates the crossentropy loss between our prediction `y_hat`</span>
   <span class=hljs-comment ># (calculated from `model(x)`) and the ground truth `y`.  We augment the data</span>
   <span class=hljs-comment ># a bit, adding gaussian random noise to our image to make it more robust.</span>
   <span class=hljs-keyword >function</span> loss(x, y)   
       x̂ = augment(x)
       ŷ = model(x̂)
       <span class=hljs-keyword >return</span> logitcrossentropy(ŷ, y)
   <span class=hljs-keyword >end</span>
  
   <span class=hljs-comment ># Train our model with the given training set using the ADAM optimizer and</span>
   <span class=hljs-comment ># printing out performance against the test set as we go.</span>
   opt = ADAM(args.lr)
  
   <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot;Beginning training loop...&quot;</span>)
   best_acc = <span class=hljs-number >0.0</span>
   last_improvement = <span class=hljs-number >0</span>
   <span class=hljs-keyword >for</span> epoch_idx <span class=hljs-keyword >in</span> <span class=hljs-number >1</span>:args.epochs
       <span class=hljs-comment ># Train for a single epoch</span>
       Flux.train!(loss, params(model), train_set, opt)
      
       <span class=hljs-comment ># Terminate on NaN</span>
       <span class=hljs-keyword >if</span> anynan(Flux.params(model))
           <span class=hljs-meta >@error</span> <span class=hljs-string >&quot;NaN params&quot;</span>
           <span class=hljs-keyword >break</span>
       <span class=hljs-keyword >end</span>
  
       <span class=hljs-comment ># Calculate accuracy:</span>
       acc = accuracy(test_set..., model)
      
       <span class=hljs-meta >@info</span>(<span class=hljs-meta >@sprintf</span>(<span class=hljs-string >&quot;[%d]: Test accuracy: %.4f&quot;</span>, epoch_idx, acc))
       <span class=hljs-comment ># If our accuracy is good enough, quit out.</span>
       <span class=hljs-keyword >if</span> acc &gt;= <span class=hljs-number >0.999</span>
           <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot; -&gt; Early-exiting: We reached our target accuracy of 99.9%&quot;</span>)
           <span class=hljs-keyword >break</span>
       <span class=hljs-keyword >end</span>
  
       <span class=hljs-comment ># If this is the best accuracy we&#x27;ve seen so far, save the model out</span>
       <span class=hljs-keyword >if</span> acc &gt;= best_acc
           <span class=hljs-meta >@info</span>(<span class=hljs-string >&quot; -&gt; New best accuracy! Saving model out to mnist_conv.bson&quot;</span>)
           BSON.<span class=hljs-meta >@save</span> joinpath(args.savepath, <span class=hljs-string >&quot;mnist_conv.bson&quot;</span>) params=cpu.(params(model)) epoch_idx acc
           best_acc = acc
           last_improvement = epoch_idx
       <span class=hljs-keyword >end</span>
  
       <span class=hljs-comment ># If we haven&#x27;t seen improvement in 5 epochs, drop our learning rate:</span>
       <span class=hljs-keyword >if</span> epoch_idx - last_improvement &gt;= <span class=hljs-number >5</span> &amp;&amp; opt.eta &gt; <span class=hljs-number >1e-6</span>
           opt.eta /= <span class=hljs-number >10.0</span>
           <span class=hljs-meta >@warn</span>(<span class=hljs-string >&quot; -&gt; Haven&#x27;t improved in a while, dropping learning rate to <span class=hljs-subst >$(opt.eta)</span>!&quot;</span>)
 
           <span class=hljs-comment ># After dropping learning rate, give it a few epochs to improve</span>
           last_improvement = epoch_idx
       <span class=hljs-keyword >end</span>
  
       <span class=hljs-keyword >if</span> epoch_idx - last_improvement &gt;= <span class=hljs-number >10</span>
           <span class=hljs-meta >@warn</span>(<span class=hljs-string >&quot; -&gt; We&#x27;re calling this converged.&quot;</span>)
           <span class=hljs-keyword >break</span>
       <span class=hljs-keyword >end</span>
   <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<p><code>train</code> calls the functions we defined above and trains our model. It stops when the model achieves 99&#37; accuracy &#40;early-exiting&#41; or after performing 20 steps. More specifically, it performs the following steps:</p>
<ul>
<li><p>Loads the MNIST dataset.</p>

<li><p>Builds our ConvNet model &#40;as described above&#41;.</p>

<li><p>Loads the train and test data sets as well as our model onto a GPU &#40;if available&#41;.</p>

<li><p>Defines a <code>loss</code> function that calculates the crossentropy between our prediction and the ground truth.</p>

<li><p>Sets the <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM">ADAM optimiser</a> to train the model with learning rate <code>args.lr</code>.</p>

<li><p>Runs the training loop. For each step &#40;or epoch&#41;, it executes the following:</p>
<ul>
<li><p>Calls <code>Flux.train&#33;</code> function to execute one training step.</p>

<li><p>If any of the parameters of our model is <code>NaN</code>, then the training process is terminated.</p>

<li><p>Calculates the model accuracy.</p>

<li><p>If the model accuracy is &gt;&#61; 0.999, then early-exiting is executed.</p>

<li><p>If the actual accuracy is the best so far, then the model is saved to <code>mnist_conv.bson</code>. Also, the new best accuracy and the current epoch is saved.</p>

<li><p>If there has not been any improvement for the last 5 epochs, then the learning rate is dropped and the process waits a little longer for the accuracy to improve.</p>

<li><p>If the last improvement was more than 10 epochs ago, then the process is terminated.</p>

</ul>

</ul>
<p>Finally, to test our model we define the <code>test</code> function: </p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> test(; kws...)
   args = Args(; kws...)
  
   <span class=hljs-comment ># Loading the test data</span>
   _,test_set = get_processed_data(args)
  
   <span class=hljs-comment ># Re-constructing the model with random initial weights</span>
   model = build_model(args)
  
   <span class=hljs-comment ># Loading the saved parameters</span>
   BSON.<span class=hljs-meta >@load</span> joinpath(args.savepath, <span class=hljs-string >&quot;mnist_conv.bson&quot;</span>) params
  
   <span class=hljs-comment ># Loading parameters onto the model</span>
   Flux.loadparams!(model, params)
  
   test_set = gpu.(test_set)
   model = gpu(model)
   <span class=hljs-meta >@show</span> accuracy(test_set...,model)
<span class=hljs-keyword >end</span></code></pre>
<p><code>test</code> loads the MNIST test data set, reconstructs the model, and loads the saved parameters &#40;in <code>mnist_conv.bson</code>&#41; onto it. Finally, it computes our model&#39;s predictions for the test set and shows the test accuracy &#40;around 99&#37;&#41;.</p>
<p>To see the full version of this example, see <a href="https://github.com/FluxML/model-zoo/blob/master/vision/conv_mnist/conv_mnist.jl">Simple ConvNets - model-zoo</a>.</p>
<h2 id=resources ><a href="#resources" class=header-anchor >Resources</a></h2>
<ul>
<li><p><a href="https://youtu.be/Oxi0Pfmskus">Neural Networks in Flux.jl with Huda Nassar &#40;working with the MNIST dataset&#41;</a></p>

<li><p><a href="https://cs231n.github.io/convolutional-networks/">Convolutional Neural Networks &#40;CNNs / ConvNets&#41;</a>.</p>

<li><p><a href="https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/">Convolutional Neural Networks Tutorial in PyTorch</a>.</p>

</ul>



<p class=author >– Elliot Saba, Adarsh Kumar, Mike J Innes, Dhairya Gandhi, Sudhanshu Agrawal, Sambit Kumar Dash, fps.io, Carlo Lucibello, Andrew Dinhobl, Liliana Badillo</p>

</div>    

    
      </div>
    </div>
    

    
    <div class="container footer lighter">
      <p>Flux: A Deep Learning Library for the Julia Programming Language
</p>
      <a href="https://twitter.com/FluxML?ref_src=twsrc%5Etfw" class=twitter-follow-button  data-show-count=false >Follow @FluxML</a>
      <script async src="https://platform.twitter.com/widgets.js" charset=utf-8 ></script>
    </div>

    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    
    
    
    <script src="/fluxml.github.io//instant.page/1.0.0" type=module  integrity="sha384-6w2SekMzCkuMQ9sEbq0cLviD/yR2HfA/+ekmKiBnFlsoSvb/VmQFSi/umVShadQI"></script>